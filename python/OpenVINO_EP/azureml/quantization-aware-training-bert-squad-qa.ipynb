{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (C) 2022, Intel Corporation\n",
        "\n",
        "SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "# Quantization Aware Training using AzureML\n",
        "\n",
        "In this sample we are demonstrating quantization aware training using Neural Networks Compression Framework.   \n",
        "Here we're using a BERT Model from the HuggingFace hub (transformers library) for Question-Answering usecase.\n",
        "\n",
        "\n",
        "This notebook is made with the reference of examples mentioned on the below links:  \n",
        "- https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-with-custom-image\n",
        "- https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Required Files\r\n",
        "\r\n",
        "We need to get the required files from the repository [here](https://github.com/intel/nlp-training-and-inference-openvino/tree/bert_qa_azureml)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "SCRIPT_DIR = 'bert_qat/'\r\n",
        "if not os.path.exists(SCRIPT_DIR):\r\n",
        "    os.mkdir(SCRIPT_DIR)\r\n",
        "if not os.path.exists(SCRIPT_DIR+\"trainer_qa.py\"):\r\n",
        "    !cd bert_qat/ && wget https://raw.githubusercontent.com/intel/nlp-training-and-inference-openvino/bert_qa_azureml/question-answering-bert-qat/quantization_aware_training/training_scripts/trainer_qa.py\r\n",
        "\r\n",
        "if not os.path.exists(SCRIPT_DIR+\"run_qa.py\"):\r\n",
        "    !cd bert_qat/ && wget https://raw.githubusercontent.com/intel/nlp-training-and-inference-openvino/bert_qa_azureml/question-answering-bert-qat/quantization_aware_training/training_scripts/run_qa.py\r\n",
        "\r\n",
        "if not os.path.exists(SCRIPT_DIR+\"utils_qa.py\"):\r\n",
        "    !cd bert_qat/ && wget https://raw.githubusercontent.com/intel/nlp-training-and-inference-openvino/bert_qa_azureml/question-answering-bert-qat/quantization_aware_training/training_scripts/utils_qa.py\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670236174499
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary Libraries\n",
        "\n",
        "First we need to import the necessary libraries to perform the desired task"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from azureml.core import Workspace\n",
        "from azureml.core import ScriptRunConfig, Experiment, Environment"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1670236174855
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Workspace\n",
        "\n",
        "The Azure Machine Learning workspace is the top-level resource for the service. It gives you a centralized place to work with all the artifacts that you create. In the Python SDK, you can access the workspace artifacts by creating a Workspace object."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "from azureml.core import Datastore\n",
        "ws = Workspace.from_config()"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1670236177007
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create or attach a compute target\n",
        "\n",
        "A Compute target is a machine where we intend to run our code. It can be a compute instance or a compute clusters.  \n",
        "Here we are using a compute cluster. If the cluster already exists it'll attach our workspace to that cluster, else it'll create a cluster according to the specification mentioned and attach to our workspace."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Choose a name for your cluster.\n",
        "cluster_name = \"cpu-cluster4\"\n",
        "\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing compute target.')\n",
        "except ComputeTargetException:\n",
        "    print('Creating a new compute target...')\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D16DS_V4',\n",
        "                                                           max_nodes=4)\n",
        "    # Create the cluster.\n",
        "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "\n",
        "    compute_target.wait_for_completion(show_output=True)\n",
        "\n",
        "# Use get_status() to get a detailed status for the current AmlCompute.\n",
        "print(compute_target.get_status().serialize())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670236178132
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting the correct input paths\n",
        "\n",
        "All scripts & files present in the `script_dir` script folder are uploaded to the compute target, data stores are mounted or copied, and the script is executed.  \n",
        "Outputs from stdout and the `./logs` folder are streamed to the run history and can be used to monitor the run. For further details please refer [here](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-pytorch#what-happens-during-run-execution)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "script_dir = os.path.abspath(SCRIPT_DIR)+\"/\"\n",
        "script_name = \"run_qa.py\""
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1670236178538
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- # Environment Definition\n",
        "\n",
        "Environment definition allows us to define a custom Docker Environment with all the required dependencies making sure our script runs as expected.\n",
        "\n",
        "## Location of the Dockerfile\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (C) 2021-2022 Intel Corporation\r\n",
        "# SPDX-License-Identifier: Apache-2.0\r\n",
        "dockerfile = r\"\"\"\r\n",
        "FROM openvino/ubuntu20_runtime:2022.2.0\r\n",
        "\r\n",
        "USER root\r\n",
        "\r\n",
        "RUN dpkg --get-selections | grep -v deinstall | awk '{print $1}' > base_packages.txt\r\n",
        "\r\n",
        "RUN apt-get update && apt-get install -y wget\\\r\n",
        "    python3.8 \\\r\n",
        "    python3.8-venv; \\\r\n",
        "    rm -rf /var/lib/apt/lists/*;\r\n",
        "\r\n",
        "RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.8 70; \\\r\n",
        "    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 70;\r\n",
        "\r\n",
        "RUN apt-get update && apt-get install -y git\r\n",
        "\r\n",
        "RUN python -m pip install --upgrade pip\r\n",
        "\r\n",
        "RUN pip install --no-cache-dir \"git+https://github.com/huggingface/optimum-intel.git@v1.5.2#egg=optimum-intel[openvino,nncf]\"\r\n",
        "RUN pip install --no-cache-dir \"git+https://github.com/AlexKoff88/nncf_pytorch.git@ak/qdq_per_channel#egg=nncf\"\r\n",
        "RUN pip install --no-cache-dir \"protobuf==3.19.4\"\r\n",
        "RUN pip install --no-cache-dir \"seqeval==1.2.2\"\r\n",
        "RUN pip install --no-cache-dir \"accelerate==0.15.0\"\r\n",
        "RUN pip install --no-cache-dir \"evaluate==0.3.0\"\r\n",
        "RUN pip install --no-cache-dir \"datasets==2.7.1\"\r\n",
        "RUN pip install --no-cache-dir \"torch==1.12.0\"\r\n",
        "RUN pip install --no-cache-dir \"openvino-dev==2022.2.0\"\r\n",
        "WORKDIR /home/training\r\n",
        "\r\n",
        "RUN chown openvino -R /home/training\r\n",
        "\r\n",
        "USER openvino\r\n",
        "\r\n",
        "RUN ls -l\r\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670236178934
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Azure ML settings\n",
        "Assigning a name to our environment for easier tracking and monitoring"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "environment_name = \"qat-example\"\n",
        "experiment_name = \"qat-test\""
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1670236179188
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Environment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = Environment(environment_name)\r\n",
        "env.docker.base_image = None\r\n",
        "env.docker.base_dockerfile = dockerfile\r\n",
        "env.python.user_managed_dependencies = True"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670236179482
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arguments for Quantization Aware Training\n",
        "\n",
        "To run the Quantization Aware Training we need to pass certain arguments to the script. The arguments can be easily passed by combining them together in a list in the below order:  \n",
        "`arguments = ['--first_arg', first_val, '--second_arg', second_val, ...]`\n",
        "\n",
        "For further details please refer [here](https://azure.github.io/azureml-cheatsheets/docs/cheatsheets/python/v1/cheatsheet/)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name =  \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "arguments = [\"--model_name_or_path\", model_name,\n",
        "             \"--dataset_name\", \"squad\",\n",
        "             \"--do_train\", True,\n",
        "             \"--do_eval\", True,\n",
        "             \"--max_seq_length\", 256,\n",
        "             \"--per_device_train_batch_size\", 3,\n",
        "             \"--max_train_samples\",10,\n",
        "             \"--max_eval_samples\",10,\n",
        "             \"--learning_rate\", 3e-5,\n",
        "             \"--num_train_epochs\", 2,\n",
        "             \"--output_dir\", \"./outputs/bert_finetuned_model\"]"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1670236179786
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create job config\n",
        "\n",
        "Job config allows us to define how we want to execute our training procedure. We need to pass the following informations to `ScriptRunConfig` object to initialize the job config instance.\n",
        "- `source_directory` $\\rightarrow$ All the contents of this source directory are copied to the compute target instance.  \n",
        "- `script` $\\rightarrow$ Our desired python script which we wish to execute  \n",
        "- `arguments` $\\rightarrow$ Necessary arguments for the `script`  \n",
        "- `env` $\\rightarrow$ Our target environment to execute the `script`\n",
        "- `compute_target` $\\rightarrow$ Our target compute preference (i.e. cluster or instance) to run the `script`"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src = ScriptRunConfig(source_directory=script_dir,\n",
        "                      script=script_name,\n",
        "                      arguments=arguments,\n",
        "                      environment=env,\n",
        "                      compute_target=cluster_name)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1670236180060
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit job\r\n",
        "After submitting the job, we can see logs from the Outputs + logs tab of the Web View link generated. Once job is completed, we can see output model from Web View link > Outputs + logs > outputs"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = Experiment(ws, experiment_name).submit(src)\n",
        "run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1669972749717
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the model outputs"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.get_file_names()\r\n",
        "print('filenames',run.get_file_names())\r\n",
        "run.download_file(name='./outputs/bert_finetuned_model/model.onnx',\r\n",
        "                 output_file_path='./models/model.onnx')\r\n",
        "run.download_file(name='./outputs/bert_finetuned_model/config.json',\r\n",
        "                 output_file_path='./models/config.json')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1669888436945
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}