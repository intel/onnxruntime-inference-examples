{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (C) 2022, Intel Corporation\n",
        "\n",
        "SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "# Onnxruntime Inference using AzureML\n",
        "\n",
        "In this sample we are running inference on a Question-Answering usecase, with the help of a quantized Bert Model using OpenVINO Execution provider.  \n",
        "\n",
        "In the following sections, we use the HuggingFace Bert model trained with Stanford Question Answering Dataset (SQuAD) dataset as an example. The Bert model is quantized with NNCF Quantize Aware Training.\n",
        "\n",
        "The question answer scenario takes a question and a piece of text called a context, and produces answer to the question extracted from the context. The questions & contexts are tokenized and encoded, fed as inputs into the transformer model. The answer is extracted from the output of the model which is the most likely start and end tokens in the context, which are then mapped back into words.\n",
        "\n",
        "# Prerequisites\n",
        "To run on AzureML, you will need:\n",
        "\n",
        "- Azure subscription\n",
        "- Azure Machine Learning Workspace (see this notebook for creation of the workspace if you do not already have one: AzureML configuration notebook)\n",
        "- the Azure Machine Learning SDK\n",
        "- the Azure CLI and the Azure Machine learning CLI extension (> version 2.2.2)\n",
        "\n",
        "The following resources can be of help:\n",
        "\n",
        "- Understand the [architecture and terms](https://learn.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-v2?tabs=cli) introduced by Azure Machine Learning\n",
        "- The [Azure Portal](https://portal.azure.com/#home) allows you to track the status of your deployments.\n",
        "\n",
        "This notebook is made with the reference of examples mentioned on the below links:  \n",
        "- [Train with custom image](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-with-custom-image)\n",
        "- [Quickstart create resources](https://learn.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Required Files\r\n",
        "We need to get the required files from the repository [here](https://github.com/intel/nlp-training-and-inference-openvino/tree/bert_qa_azureml)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "SCRIPT_DIR = \"bert_inference_onnxruntime/\"\r\n",
        "if not os.path.exists(SCRIPT_DIR):\r\n",
        "    os.mkdir(SCRIPT_DIR)\r\n",
        "\r\n",
        "if not os.path.exists(SCRIPT_DIR+\"bert_inference_optimum_ort_ovep.py\"):\r\n",
        "    !cd bert_inference_onnxruntime/ && wget https://raw.githubusercontent.com/intel/nlp-training-and-inference-openvino/bert_qa_azureml/question-answering-bert-qat/onnxovep_optimum_inference/bert_inference_optimum_ort_ovep.py\r\n",
        "if not os.path.exists(SCRIPT_DIR+\"input.csv\"):\r\n",
        "    !cd bert_inference_onnxruntime/ && wget https://raw.githubusercontent.com/intel/nlp-training-and-inference-openvino/bert_qa_azureml/question-answering-bert-qat/onnxovep_optimum_inference/data/input.csv\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670306213883
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary Libraries\r\n",
        "First we need to import the necessary libraries to perform the desired task"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from azureml.core import Workspace\n",
        "from azureml.core import ScriptRunConfig, Experiment, Environment"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1670306214257
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Workspace\n",
        "\n",
        "The Azure Machine Learning workspace is the top-level resource for the service. It gives you a centralized place to work with all the artifacts that you create. In the Python SDK, you can access the workspace artifacts by creating a Workspace object."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "ws = Workspace.from_config()"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1670306215426
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create or attach a compute target\n",
        "\n",
        "A Compute target is a machine where we intend to run our code. It can be a compute instance or a compute clusters.  \n",
        "Here we are using a compute cluster. If the cluster already exists it'll attach our workspace to that cluster, else it'll create a cluster according to the specification mentioned and attach to our workspace."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Choose a name for your cluster.\n",
        "cluster_name = \"cpu-clusters4\"\n",
        "\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing compute target.')\n",
        "except ComputeTargetException:\n",
        "    print('Creating a new compute target...')\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D16DS_V4',\n",
        "                                                           max_nodes=4)\n",
        "    # Create the cluster.\n",
        "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "\n",
        "    compute_target.wait_for_completion(show_output=True)\n",
        "\n",
        "# Use get_status() to get a detailed status for the current AmlCompute.\n",
        "print(compute_target.get_status().serialize())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1670306215797
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the Paths right\n",
        "All scripts & files present in the `script_dir` script folder are uploaded to the compute target, data stores are mounted or copied, and the script is executed.  \n",
        "Outputs from stdout and the ./logs folder are streamed to the run history and can be used to monitor the run. For further details please refer [here](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-pytorch#what-happens-during-run-execution)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "script_dir = \"bert_inference_onnxruntime/\"\n",
        "script_name = \"bert_inference_optimum_ort_ovep.py\""
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1670306216070
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Azure ML Environment Declarations\n",
        "\n",
        "Assigning a name to our environment for easier tracking and monitoring"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "environment_name = \"int8_inf-example\"\n",
        "experiment_name = \"int8_inf-test\""
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1670306216327
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Definition\n",
        "\n",
        "Environment definition allows us to define a custom Docker Environment with all the required dependencies making sure our script runs as expected."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify Docker steps as a string. \n",
        "dockerfile = r\"\"\"\n",
        "FROM openvino/ubuntu20_runtime:2022.2.0\n",
        "\n",
        "USER root\n",
        "\n",
        "RUN apt-get update && apt-get install -y \\\n",
        "    python3.8 \\\n",
        "    python3.8-venv; \\\n",
        "    rm -rf /var/lib/apt/lists/*;\n",
        "\n",
        "RUN update-alternatives --install /usr/bin/python python /usr/bin/python3.8 70; \\\n",
        "    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 70;\n",
        "\n",
        "RUN apt-get update\n",
        "RUN apt-get install -y cifs-utils\n",
        "RUN python3 -m pip install --upgrade pip\n",
        "RUN python3 -m pip install --no-cache-dir onnxruntime-openvino==1.13.1 pandas==1.5.2\n",
        "RUN python3 -m pip install --no-cache-dir optimum==1.5.1\n",
        "\n",
        "USER openvino\n",
        "\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1670306216554
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Environment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = Environment(environment_name)\n",
        "env.docker.base_image = None\n",
        "env.docker.base_dockerfile = dockerfile\n",
        "env.python.user_managed_dependencies = True"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1670306216906
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting Azure Storage for easier Access.\r\n",
        "- We need access to the corresponding keys for our storage.\r\n",
        "- The keys can be accessed from [here](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json&tabs=azure-portal#view-account-access-keys)\r\n",
        "- We mount the storage in the container for easy access of large files.\r\n",
        "\r\n",
        "## Creating job config for runnning the model\r\n",
        "\r\n",
        "\r\n",
        "Defining the set of commands that will be used to perform our desired task.  \r\n",
        "    1. Exporting some environment variables which are accessed for execution configurations during inference.  \r\n",
        "    2. Running the inference using OpenVINOExecutionProvider"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Azure Credentials\r\n",
        "Here we declare variables which will be used to connect to our Azure account. Storage account details such as storage account name and key can be accessed from :- Go to Azure portal > Storage Account >  Security + networking > Access keys. Also file share name can accessed from:- Go to corresponding file shares > Settings > Properties."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storage account name\r\n",
        "STORAGEACCT= \"\"\r\n",
        "# Storage account key\r\n",
        "STORAGEKEY = ''\r\n",
        "# Share name\r\n",
        "SHARE = ''\r\n",
        "\r\n",
        "MOUNT_PATH = \"/mnt/MyAzureFileShare\"\r\n",
        "modelname = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\r\n",
        "\r\n",
        "# Path to the Finetuned INT8 ONNX Model directory.\r\n",
        "# e.g. modelpath = f\"{MOUNT_PATH}/models\"\r\n",
        "modelpath = f\"{MOUNT_PATH}/\"\r\n"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670306217245
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Azure Storage Paths (Models & Inputs)\r\n",
        "Here we are declaring the paths to the quantized model and the inputs file.\r\n",
        "The input file is a csv file with 2 columns:\r\n",
        "- Context\r\n",
        "- Question\r\n",
        "\r\n",
        "This file will be read and inference will be performed and the corresponding outputs will be saved as `outputs.csv`."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Single Inputs\r\n",
        "\r\n",
        "You also have an option to pass sample inputs for testing.  \r\n",
        "In the below cell, the variables `context` and `question` are used to facilitate this.  \r\n",
        "These variables are passed as an argument to the inference script. If they are empty strings,\r\n",
        "then the default behaviour is to read the `inputs.csv` file, otherwise they will be processed for question answering.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------- #\r\n",
        "# Multiple User Input - using csv file\r\n",
        "# Path to Input csv file. In input csv, first field should be context and second field should be question.\r\n",
        "# e.g. inputpath = f\"{MOUNT_PATH}/input.csv\"\r\n",
        "inputpath = f\"{MOUNT_PATH}/\"\r\n",
        "# Path to Output csv file.\r\n",
        "# e.g. outputpath = f\"{MOUNT_PATH}/output.csv\"\r\n",
        "outputpath = f\"{MOUNT_PATH}/\"\r\n",
        "\r\n",
        "# -------------------------------------------------------------------------- #\r\n",
        "# Single User Input - using context and question parameters\r\n",
        "# if you are providing input csv please pass empty string to context and question(e.g context='\"\"' ,question ='\"\"')\r\n",
        "context = \"\"\" \"In its early years, the new convention center failed to meet attendance and revenue expectations.[12] By 2002, many Silicon Valley businesses were choosing the much larger Moscone Center in San Francisco over the San Jose Convention Center due to the latter's limited space. A ballot measure to finance an expansion via a hotel tax failed to reach the required two-thirds majority to pass. In June 2005, Team San Jose built the South Hall, a $6.77 million, blue and white tent, adding 80,000 square feet (7,400 m2) of exhibit space\" \"\"\"\r\n",
        "question = \"\"\" \"how may votes did the ballot measure need?\" \"\"\"\r\n",
        "# -------------------------------------------------------------------------- #\r\n",
        "\r\n",
        "provider = \"OpenVINOExecutionProvider\"\r\n",
        "\r\n",
        "command = \"\"\"\r\n",
        "mkdir -p {mount_path}\r\n",
        "mount -t cifs //{storageacct}.file.core.windows.net/{share} /mnt/MyAzureFileShare -o vers=3.0,username={storageacct},password={storagekey},dir_mode=0777,file_mode=0777,serverino\r\n",
        "\r\n",
        "ls -al {modelpath}\r\n",
        "echo Checking Model Path...\r\n",
        "ls -al {inputpath}\r\n",
        "\r\n",
        "python bert_inference_optimum_ort_ovep.py --modelname {modelname} \\\r\n",
        "    --modelpath {modelpath} \\\r\n",
        "    --provider {provider} \\\r\n",
        "    --inputpath {inputpath} \\\r\n",
        "    --outputpath {outputpath} \\\r\n",
        "    --context {context} \\\r\n",
        "    --question {question}\r\n",
        "\"\"\".format(modelpath = modelpath,\r\n",
        "           modelname = modelname,\r\n",
        "           provider = provider,\r\n",
        "           mount_path = MOUNT_PATH,\r\n",
        "           inputpath = inputpath,\r\n",
        "           outputpath = outputpath,\r\n",
        "           context=context,\r\n",
        "           question=question,\r\n",
        "           storageacct = STORAGEACCT,\r\n",
        "           storagekey = STORAGEKEY,\r\n",
        "           share = SHARE)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1670306217595
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create job config\n",
        "\n",
        "Job config allows us to define how we want to execute our training procedure. We need to pass the following informations to `ScriptRunConfig` object to initialize the job config instance.\n",
        "- `source_directory` $\\rightarrow$ All the contents of this source directory are copied to the compute target instance.  \n",
        "- `command` $\\rightarrow$ Our desired set of commands we wish to execute to perform the task.\n",
        "- `env` $\\rightarrow$ Our target environment to execute the `script`\n",
        "- `compute_target` $\\rightarrow$ Our target compute preference (i.e. cluster or instance) to run the `script`\n",
        "\n",
        "### Running the Script"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "src = ScriptRunConfig(source_directory=script_dir,\n",
        "                      command=command,\n",
        "                      environment=env,\n",
        "                      compute_target=cluster_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1670310075525
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submit job\r\n",
        "After submitting the job, we can see logs from the Outputs + logs tab of the Web View link generated."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp = Experiment(ws, experiment_name)\n",
        "run = exp.submit(src, tags={\"tag\": \"OVEP\"})\n",
        "run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "gather": {
          "logged": 1670306608804
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accessing the user output logs"
      ],
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.download_file(name=run.get_file_names()[-1], output_file_path=f'logs/'+run.get_file_names()[-1])\n",
        "with open('logs/'+run.get_file_names()[-1], 'r') as f:\n",
        "    logs = f.readlines()"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1670306610501
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing the output logs"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, log in enumerate(logs):\n",
        "    if log.startswith('Inference'):\n",
        "        break\n",
        "\n",
        "print(*logs[idx:], sep='\\n')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1670306610784
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}