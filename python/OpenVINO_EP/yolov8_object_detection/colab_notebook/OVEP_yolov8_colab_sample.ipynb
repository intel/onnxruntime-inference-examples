{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5HNjkLDZi5N"
      },
      "source": [
        "Copyright (C) 2022-2023, Intel Corporation\n",
        "\n",
        "SPDX-License-Identifier: Apache-2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7M6g-P1ZtSi"
      },
      "source": [
        "# Object detection with YOLOv8 in Python using OpenVINO™ Execution Provider:\n",
        "\n",
        "1. The Object detection sample uses a YOLOv8 Deep Learning ONNX Model.\n",
        "\n",
        "\n",
        "2. The sample involves detecting various objects in an image, which uses the OpenVINO™ Execution Provider to run inference on various Intel hardware devices as mentioned before and perform object detection to detect up to 80 different objects like person, bicycle, car, motorbike and much more from the coco dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGAysuZujJCK"
      },
      "source": [
        "First, let's make sure we're running this notebook on a supported Intel CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KeQp-9CjKz7"
      },
      "outputs": [],
      "source": [
        "# Note: This works only on Linux!\n",
        "!lscpu | grep name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4cX8riJWuNk"
      },
      "source": [
        "##Installing packages\n",
        "Getting Started\n",
        "\n",
        "In this section, we install onnxruntime-openvino, nncf, onnx, and other necessary Python packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPwBfDzCWmVX"
      },
      "outputs": [],
      "source": [
        "!python3 -m pip install --upgrade pip -q\n",
        "!python3 -m pip install onnxruntime-openvino onnx opencv-python setuptools==65.5.1 ultralytics==8.0.136 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdIH9iq4aTXt"
      },
      "source": [
        "##Downloading image for object detection and preprocessing it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5BqlWQzFbhf4"
      },
      "outputs": [],
      "source": [
        "#@title Please insert image url for object detection\n",
        "\n",
        "image_url = \"https://ultralytics.com/images/bus.jpg\" #@param [https://ultralytics.com/images/bus.jpg] {type:\"string\"}\n",
        "print(\"other images options : https://storage.openvinotoolkit.org/data/test_data/images/cat.jpg, https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYwIEFWpkFqz"
      },
      "source": [
        "Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmHT7vWLkKFy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import shutil\n",
        "import cv2\n",
        "import numpy as np\n",
        "import onnxruntime as rt\n",
        "import torch\n",
        "from statistics import mean\n",
        "from datetime import datetime\n",
        "from ultralytics.yolo.data.augment import LetterBox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvvSIzsskYbE"
      },
      "source": [
        "Downloading the image from given url and preprocessing it for object detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5B11Gp5aZKz"
      },
      "outputs": [],
      "source": [
        "# Parameters for pre-processing\n",
        "imgsz = (640,640) # default value for this usecase.\n",
        "stride = 32 # default value for this usecase( differs based on the model selected\n",
        "\n",
        "print(image_url)\n",
        "def preprocess(image_url):\n",
        "    ## Set up the image URL\n",
        "    path = os.getcwd()\n",
        "    image_path=os.path.join(path, image_url.split(\"/\")[-1])\n",
        "    # Open the url image, set stream to True, this will return the stream content.\n",
        "    r = requests.get(image_url, stream = True)\n",
        "    # Check if the image was retrieved successfully\n",
        "    if r.status_code == 200:\n",
        "        # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n",
        "        r.raw.decode_content = True\n",
        "        # Open a local file with wb ( write binary ) permission.\n",
        "        with open(image_path,'wb') as f:\n",
        "            shutil.copyfileobj(r.raw, f)\n",
        "        print('Image sucessfully downloaded: ',path)\n",
        "    else:\n",
        "        print('Image couldn\\'t be retreived')\n",
        "        return\n",
        "    image_abs_path = os.path.abspath(image_path)\n",
        "    if os.path.isfile(image_abs_path) and image_abs_path.split('.')[-1].lower() in ['jpg', 'jpeg', 'png']:\n",
        "        # Load Image\n",
        "        img0 = cv2.imread(image_abs_path)\n",
        "        # Padded resize\n",
        "        #Letterbox: Resize image and padding for detection, instance segmentation, pose\n",
        "        img = LetterBox(imgsz, stride=stride)(image=img0.copy())\n",
        "        # Convert\n",
        "        img =  img.transpose((2, 0, 1))[::-1]  # BGR to RGB, to 3x416x416\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = img.astype(np.float32)  # uint8 to fp16/32\n",
        "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        if img.ndim == 3:\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "        return img0, img\n",
        "    else:\n",
        "        print(\"Invalid image format.\")\n",
        "        return\n",
        "\n",
        "org_input, model_input = preprocess(image_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LOTNlHfhjQT"
      },
      "source": [
        "## Downloading yolov8 model and exporting it to onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vAlvihChxnv"
      },
      "outputs": [],
      "source": [
        "!mkdir -p model\n",
        "if not os.path.isfile('model/yolov8m.pt'):\n",
        "  !cd model && wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt -O yolov8m.pt\n",
        "\n",
        "# Convert pytorch version of YOLOV8 model file to onnx format\n",
        "if not os.path.isfile('model/yolov8m.onnx'):\n",
        "    !cd model && yolo mode=export model=yolov8m.pt format=onnx dynamic=True # To get a static model chnage dynamic flag to Flase\n",
        "else:\n",
        "    print(\"ONNX model file already exist. Please re-run the cell after removing it.\")\n",
        "\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btKE6yeDgj4T"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrPPbh0Qgnt-"
      },
      "outputs": [],
      "source": [
        "quantized_model_path = \"/content/model/yolov8m_quantized.onnx\"\n",
        "original_model_path = \"/content/model/yolov8m.onnx\"\n",
        "def initialize(quantize=False, device='OVEP'):\n",
        "    \"Initialize the model also getting model output and input names\"\n",
        "\n",
        "    initialized = True\n",
        "    model_dir = os.getcwd()\n",
        "    ov_model = None; mlas_model = None\n",
        "\n",
        "    so = rt.SessionOptions()\n",
        "    if device == 'OVEP':\n",
        "        if quantize == True:\n",
        "            print(\"Inferencing through OVEP\")\n",
        "            ov_model = rt.InferenceSession(quantized_model_path, so,\n",
        "                                       providers=['OpenVINOExecutionProvider'],\n",
        "                                       provider_options=[{'device_type' : 'CPU_FP32'}])\n",
        "        else:\n",
        "            ov_model = rt.InferenceSession(original_model_path, so,\n",
        "                                       providers=['OpenVINOExecutionProvider'],\n",
        "                                        provider_options=[{'device_type' : 'CPU_FP32'}])\n",
        "    elif device == 'CPUEP':\n",
        "        if quantize == True:\n",
        "            mlas_model = rt.InferenceSession(quantized_model_path, so, providers=['CPUExecutionProvider'])\n",
        "        else:\n",
        "            mlas_model = rt.InferenceSession(original_model_path, so, providers=['CPUExecutionProvider'])\n",
        "\n",
        "    if device == 'OVEP':\n",
        "      input_names = ov_model.get_inputs()[0].name\n",
        "      outputs = ov_model.get_outputs()\n",
        "    else:\n",
        "      input_names = mlas_model.get_inputs()[0].name\n",
        "      outputs = mlas_model.get_outputs()\n",
        "    output_names = list(map(lambda output:output.name, outputs))\n",
        "    return input_names, output_names, mlas_model, ov_model\n",
        "\n",
        "\n",
        "device = 'OVEP'\n",
        "input_names, output_names, mlas_model, ov_model = initialize(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqI1dSmboO1r"
      },
      "source": [
        "##Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h1tYkwGQnKtY"
      },
      "outputs": [],
      "source": [
        "#@title Select number of iterations for inference\n",
        "\n",
        "#No of iterations always greater than warmup iterations\n",
        "\n",
        "no_of_iterations = 20 #@param {type:\"slider\", min:20, max:100, step:5}\n",
        "warmup_iterations = 10 #@param {type:\"slider\", min:10, max:15, step:1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVFDmjaQoQhP"
      },
      "outputs": [],
      "source": [
        "inf_lst = []\n",
        "def inference(input_names, output_names, device, mlas_model, ovep_model, model_input):\n",
        "    if device == 'CPUEP':\n",
        "        print(\"Performing ONNX Runtime Inference with default CPU EP.\")\n",
        "        for i in range(no_of_iterations):\n",
        "          start_time = datetime.now()\n",
        "          prediction = mlas_model.run(output_names, {input_names: model_input})\n",
        "          end_time = datetime.now()\n",
        "          # print((end_time - start_time).total_seconds())\n",
        "          if i > warmup_iterations:\n",
        "            inf_lst.append((end_time - start_time).total_seconds())\n",
        "    elif device == 'OVEP':\n",
        "        print(\"Performing ONNX Runtime Inference with OpenVINO EP.\")\n",
        "        for i in range(no_of_iterations):\n",
        "          start_time = datetime.now()\n",
        "          prediction = ovep_model.run(output_names, {input_names: model_input})\n",
        "          end_time = datetime.now()\n",
        "          # print((end_time - start_time).total_seconds())\n",
        "          if i > warmup_iterations:\n",
        "            inf_lst.append((end_time - start_time).total_seconds())\n",
        "    else:\n",
        "        print(\"Invalid Device Option. Supported device options are 'cpu', 'CPU_FP32'.\")\n",
        "        return None\n",
        "    return prediction, (end_time - start_time).total_seconds()\n",
        "\n",
        "inference_output = inference(input_names, output_names, device, mlas_model, ov_model, model_input)\n",
        "# print(f'Inference time with {execution_provider} : {inference_output[1]}')\n",
        "average_inference_time = np.average(inf_lst)\n",
        "print(f'Average inference time is for {no_of_iterations - warmup_iterations} iterations is {average_inference_time} sec')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1F3WqTCxA3v"
      },
      "source": [
        "##Postprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2fxUSilxC-I"
      },
      "outputs": [],
      "source": [
        "from ultralytics.yolo.engine.results import Results\n",
        "from ultralytics.yolo.utils import ops\n",
        "from ultralytics.yolo.utils import ROOT, yaml_load\n",
        "from ultralytics.yolo.utils.checks import check_yaml\n",
        "from ultralytics.yolo.utils.plotting import Annotator, colors\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Parameters for post-processing\n",
        "conf = 0.25\n",
        "iou = 0.45\n",
        "max_det = 300\n",
        "classes = None\n",
        "agnostic = False\n",
        "labels = {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat',\n",
        "          9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat',\n",
        "          16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack',\n",
        "          25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball',\n",
        "          33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle',\n",
        "          40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich',\n",
        "          49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch',\n",
        "          58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote',\n",
        "          66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book',\n",
        "          74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
        "path = os.getcwd()\n",
        "\n",
        "def postprocess( img0, img, inference_output):\n",
        "    if inference_output is not None:\n",
        "        prediction = inference_output[0]\n",
        "        inference_time = inference_output[1]\n",
        "\n",
        "        prediction = [torch.from_numpy(pred) for pred in prediction]\n",
        "        preds = ops.non_max_suppression(prediction,\n",
        "                                                0.25,\n",
        "                                                0.45,\n",
        "                                                agnostic=agnostic,\n",
        "                                                max_det=max_det,\n",
        "                                                classes=classes)\n",
        "        log_string = ''\n",
        "        results = []\n",
        "        for _, pred in enumerate(preds):\n",
        "            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], img0.shape).round()\n",
        "            results.append(Results(img0, path, labels, boxes=pred))\n",
        "\n",
        "        det = results[0].boxes\n",
        "        if len(det) == 0:\n",
        "            return log_string+'No detection found.'\n",
        "        for c in det.cls.unique():\n",
        "            n = (det.cls == c).sum()  # detections per class\n",
        "            log_string += f\"{n} {labels[int(c)]}{'s' * (n > 1)}, \"\n",
        "\n",
        "        raw_output = ''\n",
        "        annotator = Annotator(img0, pil=False)\n",
        "        for d in reversed(det):\n",
        "            cls, conf = d.cls.squeeze(), d.conf.squeeze()\n",
        "            c = int(cls)  # integer class\n",
        "            name = f'id:{int(d.id.item())} {labels[c]}' if d.id is not None else labels[c]\n",
        "            label = f'{name} {conf:.2f}'\n",
        "            box = d.xyxy.squeeze().tolist()\n",
        "            p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
        "            raw_output+=f\"name: {name}, confidence: {conf:.2f}, start_point: {p1}, end_point:{p2}\\n\"\n",
        "            annotator.box_label(d.xyxy.squeeze(), label, color=colors(c, True))\n",
        "            # annotator.box_label(box, label, color=colors(c, True))\n",
        "\n",
        "        result_img = annotator.result()\n",
        "        # cv2.imshow('image', org_input)\n",
        "        cv2_imshow( org_input)\n",
        "        # cv2_imshow(cv2.resize(org_input, (500, 500)))\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "        return [f\"inference_time: {inference_time}s\\nInference_summary: {log_string}\\nraw_output:\\n{raw_output}\"]\n",
        "    return None\n",
        "\n",
        "result = postprocess(org_input, model_input, inference_output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Q4cX8riJWuNk"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
