{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6157254",
   "metadata": {
    "id": "b6157254"
   },
   "source": [
    "Copyright (C) 2021-2022, Intel Corporation\n",
    "\n",
    "SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6157254",
   "metadata": {
    "id": "b6157254"
   },
   "source": [
    "Major Portions of this code are copyright of their respective authors and released under the General Public License Version 3.0:\n",
    "- For licensing see https://github.com/WongKinYiu/yolov7/blob/main/LICENSE.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed77f6",
   "metadata": {
    "id": "dfed77f6"
   },
   "source": [
    "# Object detection with YOLOv7 in Python using OpenVINO™ Execution Provider:\n",
    "\n",
    "1. The Object detection sample uses a YOLOv7 Deep Learning ONNX Model.\n",
    "\n",
    "\n",
    "2. The sample involves presenting an image to ONNX Runtime (RT), which uses the OpenVINO™ Execution Provider to run inference on various Intel hardware devices as mentioned before and perform object detection to detect up to 80 different objects like person, bicycle, car, motorbike and much more from the coco dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before starting with this notebook please make sure to perform the required installations as mentioned below:**\n",
    "1. [YoloV7 installation requirements](https://github.com/WongKinYiu/yolov7#installation)\n",
    "2. [NNCF Onnx (Experimental Requirements)](https://github.com/openvinotoolkit/nncf/tree/54a444fb6ef1806c7ab8e647f762e2547ceb95bf#installation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download COCO validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "if not os.path.exists(\"../datasets\"):\n",
    "    # Download COCO val\n",
    "    torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')\n",
    "    !unzip -q tmp.zip -d ../datasets && rm tmp.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export yolov7-tiny model to ONNX format with NMS\n",
    "\n",
    "```python\n",
    "if not os.path.exists(\"yolov7-tiny.onnx\"):\n",
    "    !wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\n",
    "    !python export.py --weights yolov7-tiny.pt --grid --end2end --simplify --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640 --max-wh 640\n",
    "```\n",
    "\n",
    "**Note: Adjust the values of `iou-thres` and `conf-thres` according to requirements.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('cat.jpg'):\n",
    "    !wget https://storage.openvinotoolkit.org/data/test_data/images/cat.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference for ONNX model\n",
    "import cv2\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict,namedtuple\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define necessary helper functions\n",
    "### Pre-Processing\n",
    "\n",
    "When we are using a pre-trained model, which is trained & fine-tuned using a fixed image size as input, we should resize our image to a shape which is expected by the model. The image reshaped using a scaling factor which is a ratio between the desired height/width and the actual image height/width.  \n",
    "$$scale = min \\biggl( \\frac{\\text{target height}}{\\text{input image height}}, \\frac{\\text{target width}}{\\text{input image width}} \\biggl)$$  \n",
    "Using the $scale$-factor, image height & width are calculated which is then re-shaped to the desired image size using the `opencv` package. Here this is acheived by the `preProcess_image` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, r, (dw, dh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels and Colors for Lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', \n",
    "         'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', \n",
    "         'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', \n",
    "         'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', \n",
    "         'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', \n",
    "         'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', \n",
    "         'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', \n",
    "         'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', \n",
    "         'hair drier', 'toothbrush']\n",
    "colors = {name:[random.randint(0, 255) for _ in range(3)] for i,name in enumerate(names)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Pre-Process Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_logging(rank=-1):\n",
    "    logging.basicConfig(\n",
    "        format=\"%(message)s\",\n",
    "        level=logging.INFO if rank in [-1, 0] else logging.WARN)\n",
    "    \n",
    "set_logging(0)  # run before defining LOGGER\n",
    "LOGGER = logging.getLogger(\"yolov7\")\n",
    "\n",
    "def preProcess_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image = img.copy()\n",
    "    image, ratio, dwdh = letterbox(image, auto=False)\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    image = np.expand_dims(image, 0)\n",
    "    image = np.ascontiguousarray(image)\n",
    "\n",
    "    im = image.astype(np.float32)\n",
    "    im /= 255\n",
    "    print(\"Image Shape:\", im.shape, sep='\\t')\n",
    "    return im, ratio, dwdh\n",
    "\n",
    "def create_session(model_path, device='CPU_FP32'):\n",
    "    \n",
    "    if device == 'CPU_FP32':\n",
    "        providers = ['OpenVINOExecutionProvider']\n",
    "    elif device == 'cpu':\n",
    "        providers = ['CPUExecutionProvider']\n",
    "    else:\n",
    "        LOGGER.info(f'No provider passed, using default CPU EP ...')\n",
    "        providers = ['CPUExecutionProvider']\n",
    "    \n",
    "    # LOGGER.info(f'Use ORT providers: {providers}')\n",
    "    print(f'Use ORT providers: {providers}')\n",
    "    \n",
    "    sess = onnxruntime.InferenceSession(model_path,\n",
    "                                        providers=providers,\n",
    "                                        provider_options=[{'device_type': device}])\n",
    "\n",
    "    outname = [i.name for i in sess.get_outputs()]\n",
    "    inname = [i.name for i in sess.get_inputs()]\n",
    "    \n",
    "    return sess, outname, inname\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Inference Function\n",
    "\n",
    "The `run-inference` function takes inputs as:\n",
    "- `model_path` $\\rightarrow$ `<path to the model>.onnx`\n",
    "- `img_input` $\\rightarrow$ `<path to the image>.jpg`\n",
    "- `device` $\\rightarrow$ `device using which the inference should be run e.g. cpu, CPU_FP32`  \n",
    "    Based on the device arguement the execution providers are selected.\n",
    "\n",
    "It performs the following tasks:\n",
    "- Pre-process the input image as per model requirements\n",
    "- Creates the appropriate onnxruntime session as per the device arguement\n",
    "- Runs inferences along with non-max suppression on the predictions for the number of runs passed\n",
    "- Adds bounding boxes on the detected objects\n",
    "- Saves a copy of the inferred image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model_path, img_input, device='CPU_FP32', num_runs=10, warm_up=3):\n",
    "        \n",
    "    img0, proces_ratio, dwdh = preProcess_image(img_input)\n",
    "    session, outputs, inputs = create_session(model_path, device)\n",
    "    \n",
    "    pred_times = []\n",
    "    inp = {inputs[0]:img0}\n",
    "\n",
    "    \n",
    "    # Inference\n",
    "    for iter_num in range(num_runs + 2):\n",
    "\n",
    "        # warmup session\n",
    "        if iter_num <= warm_up:\n",
    "            session.run(outputs, inp)[0]\n",
    "            continue\n",
    "\n",
    "        start = time.time()\n",
    "        pred = session.run(outputs, inp)[0]\n",
    "        end = time.time()\n",
    "        inference_time = end - start\n",
    "        pred_times.append(inference_time)\n",
    "        \n",
    "\n",
    "    ori_images = cv2.imread(img_input)\n",
    "    ori_images = [cv2.cvtColor(ori_images, cv2.COLOR_BGR2RGB)]\n",
    "    \n",
    "    # Add bounding box to detected samples\n",
    "    for i,(batch_id,x0,y0,x1,y1,cls_id,score) in enumerate(pred):\n",
    "        image = ori_images[int(batch_id)]\n",
    "        box = np.array([x0,y0,x1,y1])\n",
    "        box -= np.array(dwdh*2)\n",
    "        box /= proces_ratio\n",
    "        box = box.round().astype(np.int32).tolist()\n",
    "        cls_id = int(cls_id)\n",
    "        score = round(float(score),3)\n",
    "        name = names[cls_id]\n",
    "        color = colors[name]\n",
    "        name += ' '+str(score)\n",
    "        cv2.rectangle(image,box[:2],box[2:],color,2)\n",
    "        cv2.putText(image,name,(box[0], box[1] - 2),cv2.FONT_HERSHEY_SIMPLEX,0.45,[225, 255, 255],thickness=1)\n",
    "        cv2.putText(image, 'FPS: {:.8f}'.format(1.0 / inference_time),\n",
    "                                  (10, 40), cv2.FONT_HERSHEY_COMPLEX, 0.45, (255, 255, 255), 0) \n",
    "    print('Avg Inference time in ms: %f' %\n",
    "          (sum(pred_times) / len(pred_times) * 1000))\n",
    "    plt.imshow(Image.fromarray(ori_images[0]))\n",
    "    plt.axis('off')\n",
    "    f = f\"{img_input.split('.')[0]}_{model_path.split('.')[0]}_{device}.jpg\"\n",
    "    plt.imsave(f, Image.fromarray(ori_images[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using CPU Execution Provider (MLAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'yolov7-tiny.onnx'\n",
    "img_path = 'cat.jpg'\n",
    "num_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"model_path\":model_path,\n",
    "          \"img_input\": img_path,\n",
    "          \"device\":'cpu',\n",
    "          \"num_runs\": num_iters}\n",
    "\n",
    "run_inference(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using OpenVino Execution Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"model_path\": model_path,\n",
    "          \"img_input\": img_path,\n",
    "          \"device\":'CPU_FP32',\n",
    "          \"num_runs\": num_iters}\n",
    "\n",
    "run_inference(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNCF PTQ for YoloV7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize yolov7-tiny model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build PTQ API dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "from nncf.experimental.post_training.compression_builder import CompressionBuilder\n",
    "from nncf.experimental.post_training.algorithms.quantization import PostTrainingQuantization\n",
    "from nncf.experimental.post_training.algorithms.quantization import PostTrainingQuantizationParameters\n",
    "from nncf.common.utils.logger import logger as nncf_logger\n",
    "from nncf.experimental.post_training.api import dataset as ptq_api_dataset\n",
    "from nncf.experimental.onnx.tensor import ONNXNNCFTensor\n",
    "from utils.datasets import LoadImagesAndLabels\n",
    "\n",
    "class YoloV7Dataset(ptq_api_dataset.Dataset):\n",
    "    def __init__(self, path, batch_size, shuffle):\n",
    "        super().__init__(batch_size, shuffle)\n",
    "        self.load_images = LoadImagesAndLabels(path)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img, _, _, _ = self.load_images[item]\n",
    "        # Input should be in [0,1].\n",
    "        img = (1 / 255.) * img\n",
    "        return {\"images\": ONNXNNCFTensor(img.numpy())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.load_images)\n",
    "\n",
    "dataset = YoloV7Dataset(\"../datasets/coco/images/val2017\", 1, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = onnx.load(\"yolov7-tiny.onnx\")\n",
    "num_init_samples = 100\n",
    "# We'll ignore detector head not to quantize them\n",
    "ignored_scopes = [\n",
    "    # Head branch 1\n",
    "    \"Mul_217\",\n",
    "    \"Add_219\",\n",
    "    \"Mul_221\",\n",
    "    \"Mul_223\",\n",
    "    \"Mul_227\",\n",
    "    # Head branch 2\n",
    "    \"Mul_251\",\n",
    "    \"Add_253\",\n",
    "    \"Mul_255\",\n",
    "    \"Mul_257\",\n",
    "    \"Mul_261\",\n",
    "    # Head branch 3\n",
    "    \"Mul_285\",\n",
    "    \"Add_287\",\n",
    "    \"Mul_289\",\n",
    "    \"Mul_291\",\n",
    "    \"Mul_295\",\n",
    "]\n",
    "output_model_path = \"yolov7-tiny-quantized.onnx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a pipeline of compression algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = CompressionBuilder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the quantization algorithm and add to the builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_parameters = PostTrainingQuantizationParameters(\n",
    "    number_samples=num_init_samples,\n",
    "    ignored_scopes=ignored_scopes\n",
    ")\n",
    "quantization = PostTrainingQuantization(quantization_parameters)\n",
    "builder.add_algorithm(quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Execute the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nncf_logger.info(\"Post-Training Quantization has just started!\")\n",
    "quantized_model = builder.apply(original_model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Save the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx.save(quantized_model, output_model_path)\n",
    "nncf_logger.info(\n",
    "    \"The quantized model is saved on {}\".format(output_model_path))\n",
    "\n",
    "onnx.checker.check_model(output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on INT-8 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using CPU Execution Provider (MLAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'yolov7-tiny-quantized.onnx'\n",
    "img_path = 'cat.jpg'\n",
    "num_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"model_path\": model_path,\n",
    "          \"img_input\": img_path,\n",
    "          \"device\":'cpu',\n",
    "          \"num_runs\": num_iters}\n",
    "\n",
    "run_inference(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using OpenVino Execution Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"model_path\": model_path,\n",
    "          \"img_input\": img_path,\n",
    "          \"device\":'CPU_FP32',\n",
    "          \"num_runs\": num_iters}\n",
    "\n",
    "run_inference(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea764a",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c93e68",
   "metadata": {},
   "source": [
    "- Benchmarks can be performed using:\n",
    "    - openvino's [benchmark app](https://github.com/openvinotoolkit/openvino/tree/master/tools/benchmark_tool)\n",
    "```\n",
    "./benchmark_app -m <path yo model>/yolov7-tiny.onnx  --shape [1,3,640,640]\n",
    "```\n",
    "    \n",
    "<!-- - `onnxruntime_perf_test` tool -->\n",
    "\n",
    "\n",
    "<!-- https://github.com/openvinotoolkit/open_model_zoo/tree/master/tools/accuracy_checker\n",
    "accuracy_check -c path/to/configuration_file -m /path/to/models -s /path/to/source/data -a /path/to/annotation -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('yv7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea635b6725f857ac32fe5a69d2a976b6cb34422ba04243780c7fe011e9e618a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
